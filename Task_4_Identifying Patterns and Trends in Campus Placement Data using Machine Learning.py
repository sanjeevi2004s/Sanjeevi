# -*- coding: utf-8 -*-
"""Complete_Identifying Patterns And Trends In Campus Placement Data Using Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15xRVr2AuncH8k2Mh5o-uwYFF0pJJotCG

## Importing the Libraries
"""

import numpy as np
import pandas as pd
import os

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
from sklearn.metrics import accuracy_score

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from tensorflow.keras import layers

"""## Read the Dataset"""

df = pd.read_csv(r"/content/collegePlace.csv")
df.head()

"""## Handling Missing Values"""

df.info()

df.isnull().sum()

"""## Handling Outliers"""

def transformationplot(feature):
  plt.figure(figsize=(12,5))
  plt.subplot(1,2,1)
  sns.distplot(feature)

transformationplot(np.log(df['Age']))

"""## Handling Catogarical Values"""

df = df.replace(['Male','Female'],[1,0])

df = df.replace(['Computer Science', 'Information Technology', 'Electronics And Communication', 'Mechanical', 'Electrical', 'Civil'], [0,1,2,3,4,5])

df = df.drop(['Hostel'], axis=1)

df.info()

df

"""## Univariate Analysis   Task - 3"""

plt.figure(figsize=(12,5))
plt.subplot(121)
sns.distplot(df['CGPA'], color='r')

plt.figure(figsize=(12,5))
plt.subplot(121)
sns.distplot(df['PlacedOrNot'], color='r')

"""##Bivariate Analysis"""

plt.figure(figsize=(18,4))
plt.subplot(1, 4, 1)
sns.countplot(df['Gender'])
plt.subplot(1, 4, 2)
sns.countplot(df['Stream'])
plt.show()

sns.countplot(x="Gender", hue="PlacedOrNot", data=df)

plt.figure(figsize=(18,4))
plt.subplot(1,4,1)
sns.countplot(x=df['Gender'])
plt.figure(figsize=(18,4))
plt.subplot(1,1,1)
sns.countplot(x=df['Stream'])
plt.show()

"""##Multivariate Analysis"""

plt.figure(figsize=(20,5))
plt.subplot(131)
sns.countplot(data=df, x="PlacedOrNot", hue="CGPA")

# create swarmplot with hue based on "Stream" column
sns.swarmplot(x=df['PlacedOrNot'], y=df['CGPA'], hue=df['Stream'])
plt.show()

"""##Scaling the Data"""

# separate features and target variable
x = df.drop(['HistoryOfBacklogs'], axis=1)
y = df['Internships']

# create a StandardScaler object
sc = StandardScaler()

# standardize the values of the features in x
x_bal = sc.fit_transform(x)

# print the standardized dataset
print(x_bal)

names = x.columns
x_bal = pd.DataFrame(x_bal,columns=names)
print(x_bal)

"""##Splitting the Data into Train and Test"""

# check the dataframe columns
print(df.columns)

# convert categorical variables to numerical using one-hot encoding
if 'Gender' in df.columns and 'Stream' in df.columns:
    df = pd.get_dummies(df, columns=['Gender', 'Stream'], drop_first=True)

# separate features and target variable
X = df.drop(['PlacedOrNot'], axis=1)

# create a StandardScaler object
scaler = StandardScaler()

# standardize the values of the features in X
standardized_data = scaler.fit_transform(X)

# assign the standardized features to X
X = standardized_data

# assign the "PlacedOrNot" target variable to Y
Y = df['PlacedOrNot']

# split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

# print the shape of the training and testing sets
print("X_train shape:", X_train.shape)
print("Y_train shape:", Y_train.shape)
print("X_test shape:", X_test.shape)
print("Y_test shape:", Y_test.shape)

"""## Milestone 4: Model Building Task_4

##SVM Model
"""

# create an SVM classifier with a linear kernel
classifier = svm.SVC(kernel='linear')

# train the classifier on the training data
classifier.fit(X_train, Y_train)

# print the accuracy of the classifier on the training and testing data
print("Training accuracy:", classifier.score(X_train, Y_train))
print("Testing accuracy:", classifier.score(X_test, Y_test))

X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data :', training_data_accuracy)

"""##KNN Model """

best_k = {"Regular":0}
best_score = {"Regular":0}
for k in range(3,50,2):
  
    ## Using Regular training set
    knn_temp = KNeighborsClassifier(n_neighbors=k)              # Instantiate the model
    knn_temp.fit(X_train, Y_train)                              # Fit the model to the training set
    knn_temp_pred = knn_temp.predict(X_test)                    # Predict on the test set
    score = metrics.accuracy_score(Y_test, knn_temp_pred)*100   # Get accuracy
    if score >= best_score["Regular"] and score < 100:          # Store best params
        best_score["Regular"] = score
        best_k["Regular"] = k

print("---Results---\nK: {}\nScore: {}".format(best_k, best_score))

## Instantiate the Models
knn = KNeighborsClassifier(n_neighbors=best_k["Regular"])

## Fit the Model to the Training Set
knn.fit(X_train, Y_train)
knn_pred = knn.predict(X_test)
testd = accuracy_score(knn_pred, Y_test)

"""##Artificial Neural Network Model"""

classifier = Sequential()

# Add input layer and first hidden layer
classifier.add(keras.layers.Dense(10, activation='relu', input_dim=10))
classifier.add(keras.layers.Dropout(0.50))

# Add 2nd hidden layer
classifier.add(keras.layers.Dense(10, activation='relu'))
classifier.add(keras.layers.Dropout(0.50))

# Final or output layer
classifier.add(keras.layers.Dense(1, activation='sigmoid'))

# Compiling the model
loss_1 = tf.keras.losses.BinaryCrossentropy()
classifier.compile(optimizer='Adam', loss=loss_1, metrics=['accuracy'])

# Fitting the model
classifier.fit(X_train, Y_train, batch_size=20, epochs=100)